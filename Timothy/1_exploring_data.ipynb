{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import RecordVideo, FrameStack\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0, FrameStackObservationV0\n",
    "from gymnasium.utils.play import play"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Basic Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sets the precision to 3 decimal places.\n",
    "numpy.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"ALE/SpaceInvaders-v5\", render_mode='rgb_array')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Observation Space in the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension, 210, represents the height of the observation image. It indicates that the observation is an image with a height of 210 pixels.\n",
    "\n",
    "The second dimension, 160, represents the width of the observation image. It indicates that the observation is an image with a width of 160 pixels.\n",
    "\n",
    "The third dimension, 3, represents the number of color channels in the observation image. In this case, 3 indicates that the observation image is in RGB format, meaning it has three color channels: red, green, and blue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reductions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gray Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grayscale_env = GrayscaleObservationV0(env)\n",
    "grayscale_env.observation_space.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It indicates that the observation space is a 2-dimensional array.\n",
    "\n",
    "The first dimension, 210, represents the height of the grayscale observation image. It indicates that the grayscale observation is an image with a height of 210 pixels.\n",
    "\n",
    "The second dimension, 160, represents the width of the grayscale observation image. It indicates that the grayscale observation is an image with a width of 160 pixels.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrameSkip (Frameskip 4 is default)\n",
    "\n",
    "Frame skipping is commonly used in reinforcement learning with Atari games to speed up training and reduce the computational requirements. By skipping frames, the agent's action is applied to the environment for every n-th frame, where n is the skip value.\n",
    "\n",
    "There are a few reasons why frame skipping is used:\n",
    "\n",
    "1. Reduced computation: Skipping frames allows the agent to process fewer frames per second, reducing the computational requirements and speeding up training. Atari games typically run at 60 frames per second, and processing all frames can be computationally expensive.\n",
    "2. Temporal abstraction: Frame skipping provides a form of temporal abstraction by aggregating multiple frames into a single action. This can help the agent capture motion patterns and make decisions based on the movement of objects in the game.\n",
    "3. Reacting to changing states: Skipping frames allows the agent to react to changing states in the game more quickly. If the agent only processed every frame, it might miss important changes in the environment between consecutive frames.\n",
    "4. Exploration: Frame skipping can improve exploration by allowing the agent to take a wider range of actions and explore different game states more efficiently. It helps to prevent the agent from getting stuck in repetitive actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render_modes': ['human', 'rgb_array'], 'obs_types': {'rgb', 'grayscale', 'ram'}}\n"
     ]
    }
   ],
   "source": [
    "env = gymnasium.make(\"ALE/SpaceInvaders-v5\", render_mode='rgb_array')\n",
    "print(env.metadata)\n",
    "#env.metadata['frame_skip'] = 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Data for Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framestack (A form of log()the images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "frameStack = FrameStackObservationV0(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 210, 160, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frameStack.observation_space.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It indicates that the observation space is a 4-dimensional array with a shape of (4, 210, 160, 3).\n",
    "\n",
    " Let's break down the meaning of each dimension:\n",
    " \n",
    "1. The first dimension, $4$, represents the number of frames stacked together. It indicates that the observation consists of a stack of 4 consecutive frames.\n",
    "2. The second dimension, $210$, represents the height of each frame in the stack. It indicates that each frame has a height of 210 pixels.\n",
    "3. The third dimension, $160$, represents the width of each frame in the stack. It indicates that each frame has a width of 160 pixels.\n",
    "4. The fourth dimension, $3$, represents the number of color channels in each frame. It indicates that each frame has three color channels (red, green, and blue)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fram Stacking vs Frame Skipping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame stacking and frame skipping are two different techniques used in reinforcement learning to process and utilize sequential observations from an environment. Here's an explanation of each technique:\n",
    "1. Frame Stacking:\n",
    "- Frame stacking involves creating a stack of consecutive frames as the input observation for the agent. Instead of providing a single frame at each time step, multiple frames are stacked together.\n",
    "- Stacking frames helps to capture temporal information and provide a sense of motion to the agent. By considering multiple frames in sequence, the agent can observe changes over time and make more informed decisions.\n",
    "- The stacked frames are typically used as input to the agent's neural network. The depth of the stack determines how many previous frames the agent can see and utilize for decision making.\n",
    "- Frame stacking is useful in scenarios where the dynamics and motion in the environment play an important role, such as in video games or tasks with fast-paced movements.\n",
    "2. Frame Skipping:\n",
    "- Frame skipping involves skipping a certain number of frames between each consecutive observation given to the agent. Instead of processing every frame, only a subset of frames is considered.\n",
    "- Skipping frames helps to reduce the computational load and accelerate training by reducing the number of environment steps taken.\n",
    "- The skipped frames are typically ignored in terms of observation and action selection. The agent only receives observations and takes actions based on the frames that are not skipped.\n",
    "- Frame skipping is useful in scenarios where high-frequency actions or changes in the environment may not be necessary to process at every time step. Skipping frames can help in situations where the relevant information is spread out across longer time intervals.\n",
    "\n",
    "It's important to note that frame stacking and frame skipping can be used together in reinforcement learning algorithms. Frame stacking captures temporal information within each observation, while frame skipping reduces computational overhead by processing fewer frames. These techniques can be combined to provide a more efficient and informative representation of the environment to the agent."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaceInvaders has the action space Discrete(6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Value | Meaning    |\n",
    "|-------|------------|\n",
    "| 0     | NOOP       |\n",
    "| 1     | FIRE       |\n",
    "| 2     | RIGHT      |\n",
    "| 3     | LEFT       |\n",
    "| 4     | RIGHTFIRE  |\n",
    "| 5     | LEFTFIRE   |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV reset variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, info = grayscale_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 210 entries, 0 to 209\n",
      "Columns: 160 entries, 0 to 159\n",
      "dtypes: uint8(160)\n",
      "memory usage: 32.9 KB\n"
     ]
    }
   ],
   "source": [
    "obs = pd.DataFrame(obs)\n",
    "obs.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printed 'obs' represents the observation returned by the environment after taking a random action. Here's how to interpret the shape and values of the 'obs' array:\n",
    "- The shape of 'obs' is '(height, width, channels) \", where:\n",
    "- \"height 'refers to the number of rows in the observation image.\n",
    "- 'width' refers to the number of columns in the observation image.\n",
    "- 'channels\" refers to the number of color channels in the observation image (e.g., RGB images have 3 channels).\n",
    "- Each element of the 'obs' array represents the pixel values of the observation image. In this case, it appears to be a grayscale image since each pixel value is a single integer.\n",
    "- The pixel values range from 0 to 255 , representing the intensity or brightness of each pixel. In your example, most of the pixel values seem to be 0 , indicating a black or dark region in the image.\n",
    "- The array is structured as a 3-dimensional NumPy array, with each dimension representing a different aspect of the observation image: rows, columns, and channels.\n",
    "- The printed representation shows a subset of the pixels from the observation image. Each row represents a horizontal line of pixels in the image, and each column represents a vertical column of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lives': 3, 'episode_frame_number': 0, 'frame_number': 0}\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENV step variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResetNeeded",
     "evalue": "Cannot call env.step() before calling env.reset()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResetNeeded\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample() \u001b[39m# Just a random agent\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI_Mac/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:55\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Steps through the environment with `kwargs`.\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[0;31mResetNeeded\u001b[0m: Cannot call env.step() before calling env.reset()"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample() # Just a random agent\n",
    "obs, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  ...\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]]\n",
      "\n",
      " [[80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  ...\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]]\n",
      "\n",
      " [[80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  ...\n",
      "  [80 89 22]\n",
      "  [80 89 22]\n",
      "  [80 89 22]]]\n"
     ]
    }
   ],
   "source": [
    "print(obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI gets reward everytime it shoots an alien, reward is accumlated as the total score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lives': 3, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    }
   ],
   "source": [
    "print(info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To add diffculty to the game we must declar it, 0 is default difculty or 1 is\n",
    "difficulty=1\n",
    "\n",
    "env = gymnasium.make(\"ALE/SpaceInvaders-v5\", render_mode='rgb_array', difficulty = difficulty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
