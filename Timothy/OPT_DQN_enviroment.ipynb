{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple, deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0, FrameStackObservationV0\n",
    "from tensorflow.keras.losses import MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sets the precision to 3 decimal places.\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Setup (Mode 2 is the real game)\n",
    "Game Mode 0 (default) - 15\n",
    "\n",
    "Changing the Game Mode (Good For Testing Generalisation of the model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit3ct/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SpaceInvaders-ram-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "mode = 0\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\", mode=mode)\n",
    "env.metadata['render_fps'] = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "undestanding the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "ram_space = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(ram_space)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confirmed Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" episodes = 100\\n\\nfor episode in range(0, episodes+1):\\n    env.close()\\n    state = env.reset()\\n    done = False\\n    score = 0 \\n    \\n    while not done:\\n        action = random.choice([0,1,2,3,4,5])\\n        new_state, reward, done, _, info = env.step(action)\\n        score+=reward\\n    \\n    if(episode > 0):\\n        print('Episode:{} Score:{}'.format(episode, score))\\n \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" episodes = 100\n",
    "\n",
    "for episode in range(0, episodes+1):\n",
    "    env.close()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        score+=reward\n",
    "    \n",
    "    if(episode > 0):\n",
    "        print('Episode:{} Score:{}'.format(episode, score))\n",
    " \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Deep Q-Learning\n",
    "\n",
    "We discovered with Q-learning that it was not possible to train the agent with the Q-Learning algorithm due to the limitations of the Q-Table. The state returned by env.reset() and env.step(action) in the Atari environments is a full image from the game screen. Pixel values range from 0 to 255, and most of them will not correspond to valid indices in your Q-table.\n",
    "\n",
    "Therefore we will utilize Deep Q-Learning to solve this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"elu\", input_shape=ram_space),\n",
    "    tf.keras.layers.Dense(64, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(actions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,806\n",
      "Trainable params: 12,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation=\"elu\", input_shape=ram_space),\n",
    "    tf.keras.layers.Dense(64, activation=\"elu\"),\n",
    "    tf.keras.layers.Dense(actions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,806\n",
      "Trainable params: 12,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_q_net.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 0.001 # learning rate\n",
    "TAU = 0.001 # soft update factor\n",
    "\n",
    "STEPS_BETWEEN_LEARNING = 4\n",
    "MINI_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(experiences, gamma, q_net, target_q_net):\n",
    "    '''\n",
    "    y_j = R_j if episode terminates at T_{j+1}, else\n",
    "    y_j = R_j + gamma max_{a'} Q^(s_{j+1}, a')\n",
    "    '''\n",
    "    # unpack experiences into its components\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_net(next_states), axis=-1)\n",
    "    \n",
    "    # y = R if episode terminates, else y = R + y max Q^(s,a)\n",
    "    # note: done_vals is boolean, and (1 - done_vals) == 0 if done_vals == True\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    \n",
    "    q_values = q_net(states)\n",
    "    # tf.gather_nd(params, indices) returns `indices` slices of `params`\n",
    "    q_values = tf.gather_nd(q_values, \n",
    "                            tf.stack([tf.range(q_values.shape[0]),\n",
    "                                      tf.cast(actions, tf.int32)],\n",
    "                                     axis=1\n",
    "                                    ))\n",
    "    \n",
    "    # compute MSE loss\n",
    "    return MSE(y_targets, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Fit the Q and Q target networks\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        computed_loss = loss(experiences, gamma, q_net, target_q_net)\n",
    "        \n",
    "    # update q_net\n",
    "    gradients = tape.gradient(computed_loss, q_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, q_net.trainable_variables))\n",
    "    \n",
    "    # update target_q_net\n",
    "    for target_weights, q_net_weights in zip(target_q_net.weights, q_net.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 15:57:57.956099: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 | Total score average of last 100 episodes: 132.07"
     ]
    }
   ],
   "source": [
    "time_begin = time.time()\n",
    "\n",
    "episodes = 10_000 # how many times we play the game\n",
    "max_steps = 1000 # how many actions allowed before 'time out'\n",
    "avg_latest_score_before_save = 400\n",
    "\n",
    "score_history = []\n",
    "\n",
    "avg_points = 100\n",
    "\n",
    "epsilon = 1.0 # initial epsilon for greedy policy (exploration/exploitation)\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# clone q_net into target_q_net\n",
    "target_q_net.set_weights(q_net.get_weights())\n",
    "\n",
    "for i in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        if(t <= max_steps):\n",
    "            t += 1\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = q_net(state_qn)\n",
    "        # make epsilon greedy choice\n",
    "        if random.random() > epsilon:\n",
    "            # exploitation\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            # exploration\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # take action and update buffer\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # check whether this is a learning step\n",
    "        if (t + 1) % STEPS_BETWEEN_LEARNING == 0 and len(buffer) > MINI_BATCH_SIZE:\n",
    "            # this is a learning step\n",
    "            experiences = random.sample(buffer, k=MINI_BATCH_SIZE)\n",
    "            states = tf.convert_to_tensor(\n",
    "                np.array([e.state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(\n",
    "                np.array([e.action for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(\n",
    "                np.array([e.reward for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(\n",
    "                np.array([e.next_state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            done_vals = tf.convert_to_tensor(\n",
    "                np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                dtype=tf.float32)\n",
    "            experiences = states, actions, rewards, next_states, done_vals\n",
    "            learn(experiences, GAMMA)\n",
    "            \n",
    "        state = next_state.copy()\n",
    "        score += reward\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "    score_history.append(score)\n",
    "    avg_latest_score = np.mean(score_history[-avg_points:])\n",
    "\n",
    "    # update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # display status\n",
    "    status_message = f'\\rEpisode {i + 1} | Total score average of last {avg_points} episodes: {avg_latest_score:.2f}'\n",
    "    print(status_message, end='')\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(status_message)\n",
    "\n",
    "    if avg_latest_score >= avg_latest_score_before_save:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_net.save('crazy_astronaut.h5')\n",
    "            \n",
    "time_end = time.time()\n",
    "runtime = time_end - time_begin\n",
    "print(f'\\nTotal runtime: {runtime:.2f} sec. ({(runtime/60):.2f} min.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m      2\u001b[0m plt\u001b[39m.\u001b[39mplot(score_history)\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m\"\u001b[39m\u001b[39mEpisode\u001b[39m\u001b[39m\"\u001b[39m, fontsize\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(score_history)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, training an AI on a game like space invaders takes a long time, due to the fact as the agent surives longer it requires time and energy.\n",
    "\n",
    "There could utilised different strategies to optimize this process and lowering dimension og set scope to someling like (highest score) in shortes amount of time, as this would like triggers the AI to seek to kill the aliens faster and this would also transition to surviving longer when competing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Setup (this need to point the trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(env):\n",
    "    \"\"\"Random agent that samples actions from the environment's action space.\"\"\"\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1960065885.py, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[134], line 28\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Highest Score: {highest_score}\") \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\"\"\" def get_agent_scores(num_episodes = 1):\n",
    "    scores = pandas.DataFrame(columns=['score'])\n",
    "    total_reward = 0\n",
    "    #highest_score = 0\n",
    "    \n",
    "    for _ in range(0, num_episodes + 1):\n",
    "        #if(_ > 1 and total_reward > 0):\n",
    "        #    print(f'Game Episode {_ - 1}: Score: {total_reward}')\n",
    "        env.close()\n",
    "        total_reward = 0\n",
    "        observation, info = env.reset()  # Getting start stats from the game\n",
    "        lives = info['lives']  # Initialize lives inside the loop\n",
    "        while lives != 0:\n",
    "            # For demonstration purposes, we use a random agent\n",
    "            action = agent(env)   # Just a random agent\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            lives = info['lives']\n",
    "            total_reward = total_reward + reward\n",
    "\n",
    "        # Update highest score if current total reward is higher\n",
    "        #if total_reward > highest_score:\n",
    "        #    highest_score = total_reward\n",
    "\n",
    "        scores = pandas.concat([scores, pandas.DataFrame({'score': [total_reward]})], ignore_index=True)\n",
    "\n",
    "\n",
    "    \"\"\" # Print the highest score achieved\n",
    "    print(f\"Highest Score: {highest_score}\") \"\"\"\n",
    "\n",
    "    # Close the environment to finalize the video recording\n",
    "    scores = scores.drop(scores.index[0])\n",
    "    return scores\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4.\n",
      "Moviepy - Writing video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4\n",
      "Moviepy - Building video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4.\n",
      "Moviepy - Writing video /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video/rl-video-episode-20.mp4\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "scores = get_agent_scores(num_episodes = num_episodes)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics of Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 1 to 100\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   score   100 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 932.0 bytes\n"
     ]
    }
   ],
   "source": [
    "scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score\n",
       "1  165.0\n",
       "2   35.0\n",
       "3   35.0\n",
       "4  110.0\n",
       "5   45.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score\n",
       "96    90.0\n",
       "97   160.0\n",
       "98    35.0\n",
       "99    90.0\n",
       "100  125.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>155.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>117.553974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>125.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>185.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>650.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score\n",
       "count  100.000000\n",
       "mean   155.050000\n",
       "std    117.553974\n",
       "min     10.000000\n",
       "25%     80.000000\n",
       "50%    125.000000\n",
       "75%    185.000000\n",
       "max    650.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode 1: Score 165.0\n",
      "Game Episode 100: Score 125.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "index_start = 0\n",
    "index_end = num_episodes-1\n",
    "\n",
    "print(f\"Game Episode 1: Score {scores.iloc[index_start]['score']}\\nGame Episode {num_episodes}: Score {scores.iloc[index_end]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest Score: 10.0\n",
      "High Score: 650.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lowest Score: {scores['score'].min()}\\nHigh Score: {scores['score'].max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode (most frequent element)\n",
    "It is possible for a sample to have multiple modes. In statistics, a mode refers to the value or values in a dataset that occur most frequently. If there are multiple values with the same highest frequency, the dataset is considered multimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score\n",
       "0  155.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median (middle value of the data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    125.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    155.05\n",
       "dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance (Dissimilarity between samples)\n",
    "\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "Note: The number is high due there is a lot of Dissimilarities between each score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma=\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}\\text{ for samples (degrees of freedom is 0)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    13680.7475\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.var(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation ( variability within a sample)\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "$$SD_0=\\sqrt{\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}}\\text{ for samples (degrees of freedom is 0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score    116.964728\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores.std(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16af07c2815b4438991983ee407b8d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d04175c3aa34304aafd3f14887874e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324380071c87491bad53bab85f25f116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518d5009f6c44b919b65fdb79156a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report = pandas_profiling.ProfileReport(scores).to_file('test_agent_scores_report.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
