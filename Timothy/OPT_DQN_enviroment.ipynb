{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple, deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0, FrameStackObservationV0\n",
    "from tensorflow.keras.losses import MSE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sets the precision to 3 decimal places.\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Setup (Mode 2 is the real game)\n",
    "Game Mode 0 (default) - 15\n",
    "\n",
    "Changing the Game Mode (Good For Testing Generalisation of the model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit3ct/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SpaceInvaders-ram-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "mode = 0\n",
    "env = gym.make(\"SpaceInvaders-ram-v0\", mode=mode)\n",
    "env.metadata['render_fps'] = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Insights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "undestanding the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,)\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "ram_space = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "print(ram_space)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confirmed Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" episodes = 100\\n\\nfor episode in range(0, episodes+1):\\n    env.close()\\n    state = env.reset()\\n    done = False\\n    score = 0 \\n    \\n    while not done:\\n        action = random.choice([0,1,2,3,4,5])\\n        new_state, reward, done, _, info = env.step(action)\\n        score+=reward\\n    \\n    if(episode > 0):\\n        print('Episode:{} Score:{}'.format(episode, score))\\n \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" episodes = 100\n",
    "\n",
    "for episode in range(0, episodes+1):\n",
    "    env.close()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        score+=reward\n",
    "    \n",
    "    if(episode > 0):\n",
    "        print('Episode:{} Score:{}'.format(episode, score))\n",
    " \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Deep Q-Learning\n",
    "\n",
    "We discovered with Q-learning that it was not possible to train the agent with the Q-Learning algorithm due to the limitations of the Q-Table. The state returned by env.reset() and env.step(action) in the Atari environments is a full image from the game screen. Pixel values range from 0 to 255, and most of them will not correspond to valid indices in your Q-table.\n",
    "\n",
    "Therefore we will utilize Deep Q-Learning to solve this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Learning Model with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", input_shape=ram_space),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(actions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,598\n",
      "Trainable params: 42,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_net = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", input_shape=ram_space),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(actions)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                2112      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               33280     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,598\n",
      "Trainable params: 42,598\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_q_net.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1_000_000\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 0.001 # learning rate\n",
    "TAU = 0.001 # soft update factor\n",
    "\n",
    "STEPS_BETWEEN_LEARNING = 4\n",
    "MINI_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(experiences, gamma, q_net, target_q_net):\n",
    "    '''\n",
    "    y_j = R_j if episode terminates at T_{j+1}, else\n",
    "    y_j = R_j + gamma max_{a'} Q^(s_{j+1}, a')\n",
    "    '''\n",
    "    # unpack experiences into its components\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_net(next_states), axis=-1)\n",
    "    \n",
    "    # y = R if episode terminates, else y = R + y max Q^(s,a)\n",
    "    # note: done_vals is boolean, and (1 - done_vals) == 0 if done_vals == True\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    \n",
    "    q_values = q_net(states)\n",
    "    # tf.gather_nd(params, indices) returns `indices` slices of `params`\n",
    "    q_values = tf.gather_nd(q_values, \n",
    "                            tf.stack([tf.range(q_values.shape[0]),\n",
    "                                      tf.cast(actions, tf.int32)],\n",
    "                                     axis=1\n",
    "                                    ))\n",
    "    \n",
    "    # compute MSE loss\n",
    "    return MSE(y_targets, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Fit the Q and Q target networks\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        computed_loss = loss(experiences, gamma, q_net, target_q_net)\n",
    "        \n",
    "    # update q_net\n",
    "    gradients = tape.gradient(computed_loss, q_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, q_net.trainable_variables))\n",
    "    \n",
    "    # update target_q_net\n",
    "    for target_weights, q_net_weights in zip(target_q_net.weights, q_net.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 16:27:06.046449: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 | Total score average of last 100 episodes: 128.50, steps: 595.8, epsilon: 0.9046104802746175562615825302"
     ]
    }
   ],
   "source": [
    "time_begin = time.time()\n",
    "\n",
    "episodes = 10_000 # how many times we play the game\n",
    "max_steps = 4_000_0000 # how many actions allowed before 'time out'\n",
    "avg_latest_score_before_save = 230\n",
    "\n",
    "score_history = []\n",
    "steps_hisory = []\n",
    "\n",
    "avg_points = 100\n",
    "\n",
    "epsilon = 1.0 # initial epsilon for greedy policy (exploration/exploitation)\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# clone q_net into target_q_net\n",
    "target_q_net.set_weights(q_net.get_weights())\n",
    "\n",
    "for i in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        if(t <= max_steps):\n",
    "            t += 1\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = q_net(state_qn)\n",
    "        # make epsilon greedy choice\n",
    "        if random.random() > epsilon:\n",
    "            # exploitation\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            # exploration\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # take action and update buffer\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # check whether this is a learning step\n",
    "        if (t + 1) % STEPS_BETWEEN_LEARNING == 0 and len(buffer) > MINI_BATCH_SIZE:\n",
    "            # this is a learning step\n",
    "            experiences = random.sample(buffer, k=MINI_BATCH_SIZE)\n",
    "            states = tf.convert_to_tensor(\n",
    "                np.array([e.state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(\n",
    "                np.array([e.action for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(\n",
    "                np.array([e.reward for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(\n",
    "                np.array([e.next_state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            done_vals = tf.convert_to_tensor(\n",
    "                np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                dtype=tf.float32)\n",
    "            experiences = states, actions, rewards, next_states, done_vals\n",
    "            learn(experiences, GAMMA)\n",
    "            \n",
    "        state = next_state.copy()\n",
    "        score += reward\n",
    "        \n",
    "        if done: break\n",
    "        \n",
    "    score_history.append(score)\n",
    "    steps_hisory.append(t)\n",
    "    avg_latest_score = np.mean(score_history[-avg_points:])\n",
    "    avg_latest_steps = np.mean(steps_hisory[-avg_points:])\n",
    "\n",
    "    # update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # display status\n",
    "    status_message = f'\\rEpisode {i + 1} | Total score average of last {avg_points} episodes: {avg_latest_score:.2f}, steps: {avg_latest_steps}, epsilon: {epsilon}'\n",
    "    print(status_message, end='')\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(status_message)\n",
    "\n",
    "    if avg_latest_score >= avg_latest_score_before_save:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_net.save('ram_crazy_astronaut.h5')\n",
    "            \n",
    "time_end = time.time()\n",
    "runtime = time_end - time_begin\n",
    "print(f'\\nTotal runtime: {runtime:.2f} sec. ({(runtime/60):.2f} min.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_net.save('ram_crazy_astronaut.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(score_history)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, training an AI on a game like space invaders takes a long time, due to the fact as the agent surives longer it requires time and energy.\n",
    "\n",
    "There could utilised different strategies to optimize this process and lowering dimension og set scope to someling like (highest score) in shortes amount of time, as this would like triggers the AI to seek to kill the aliens faster and this would also transition to surviving longer when competing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Setup (this need to point the trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(env):\n",
    "    \"\"\"Random agent that samples actions from the environment's action space.\"\"\"\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def get_agent_scores(num_episodes = 1):\n",
    "    scores = pandas.DataFrame(columns=['score'])\n",
    "    total_reward = 0\n",
    "    #highest_score = 0\n",
    "    \n",
    "    for _ in range(0, num_episodes + 1):\n",
    "        #if(_ > 1 and total_reward > 0):\n",
    "        #    print(f'Game Episode {_ - 1}: Score: {total_reward}')\n",
    "        env.close()\n",
    "        total_reward = 0\n",
    "        observation, info = env.reset()  # Getting start stats from the game\n",
    "        lives = info['lives']  # Initialize lives inside the loop\n",
    "        while lives != 0:\n",
    "            # For demonstration purposes, we use a random agent\n",
    "            action = agent(env)   # Just a random agent\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            lives = info['lives']\n",
    "            total_reward = total_reward + reward\n",
    "\n",
    "        # Update highest score if current total reward is higher\n",
    "        #if total_reward > highest_score:\n",
    "        #    highest_score = total_reward\n",
    "\n",
    "        scores = pandas.concat([scores, pandas.DataFrame({'score': [total_reward]})], ignore_index=True)\n",
    "\n",
    "\n",
    "    \"\"\" # Print the highest score achieved\n",
    "    print(f\"Highest Score: {highest_score}\") \"\"\"\n",
    "\n",
    "    # Close the environment to finalize the video recording\n",
    "    scores = scores.drop(scores.index[0])\n",
    "    return scores\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "scores = get_agent_scores(num_episodes = num_episodes)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics of Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_start = 0\n",
    "index_end = num_episodes-1\n",
    "\n",
    "print(f\"Game Episode 1: Score {scores.iloc[index_start]['score']}\\nGame Episode {num_episodes}: Score {scores.iloc[index_end]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lowest Score: {scores['score'].min()}\\nHigh Score: {scores['score'].max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode (most frequent element)\n",
    "It is possible for a sample to have multiple modes. In statistics, a mode refers to the value or values in a dataset that occur most frequently. If there are multiple values with the same highest frequency, the dataset is considered multimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median (middle value of the data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance (Dissimilarity between samples)\n",
    "\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "Note: The number is high due there is a lot of Dissimilarities between each score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma=\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}\\text{ for samples (degrees of freedom is 0)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.var(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation ( variability within a sample)\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "$$SD_0=\\sqrt{\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}}\\text{ for samples (degrees of freedom is 0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores.std(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pandas_profiling.ProfileReport(scores).to_file('test_agent_scores_report.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
