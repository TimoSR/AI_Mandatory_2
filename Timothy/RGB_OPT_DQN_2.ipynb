{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pandas_profiling\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple, deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gymnasium.experimental.wrappers import GrayscaleObservationV0, FrameStackObservationV0\n",
    "from tensorflow.keras.losses import MSE \n",
    "from keras.layers.convolutional import Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sets the precision to 3 decimal places.\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enviroment Setup (Mode 2 is the real game)\n",
    "Game Mode 0 (default) - 15\n",
    "\n",
    "Changing the Game Mode (Good For Testing Generalisation of the model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit3ct/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/gymnasium/envs/registration.py:523: DeprecationWarning: \u001b[33mWARN: The environment SpaceInvaders-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "mode = 2\n",
    "env = gymnasium.make(\"SpaceInvaders-v0\", render_mode='rgb_array', mode=mode)\n",
    "env.metadata['frame_skip'] = 4\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GrayscaleObservationV0(env)\n",
    "#env = FrameStackObservationV0(env, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recording Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archit3ct/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/archit3ct/Code/Assignments/AI_Mandatory_2/Timothy/deep-q-learning_video folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"deep-q-learning_video\"\n",
    "video_output_frequency = 20\n",
    "env = RecordVideo(env, output_dir, episode_trigger=lambda episode_id: episode_id == 1 or (episode_id != 0 and (episode_id) % video_output_frequency == 0))\n",
    "env.metadata['render_fps'] = 24"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undestanding the dimensions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "states = env.observation_space.shape\n",
    "height, width = env.observation_space.shape\n",
    "actions = env.action_space.n\n",
    "#print(framestack, height, width)\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Env"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Confirmed Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" episodes = 100\\n\\nfor episode in range(0, episodes+1):\\n    env.close()\\n    state = env.reset()\\n    done = False\\n    score = 0 \\n    \\n    while not done:\\n        env.render()\\n        action = random.choice([0,1,2,3,4,5])\\n        new_state, reward, done, _, info = env.step(action)\\n        score+=reward\\n    \\n    if(episode > 0):\\n        print('Episode:{} Score:{}'.format(episode, score))\\n \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" episodes = 100\n",
    "\n",
    "for episode in range(0, episodes+1):\n",
    "    env.close()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([0,1,2,3,4,5])\n",
    "        new_state, reward, done, _, info = env.step(action)\n",
    "        score+=reward\n",
    "    \n",
    "    if(episode > 0):\n",
    "        print('Episode:{} Score:{}'.format(episode, score))\n",
    " \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Deep Q-Learning\n",
    "\n",
    "We discovered with Q-learning that it was not possible to train the agent with the Q-Learning algorithm due to the limitations of the Q-Table. The state returned by env.reset() and env.step(action) in the Atari environments is a full image from the game screen. Pixel values range from 0 to 255, and most of them will not correspond to valid indices in your Q-table.\n",
    "\n",
    "Therefore we will utilize Deep Q-Learning to solve this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Deep Learning Model with Keras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try use an CNN with grayscale aka 1 the 1 in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(states[0],states[1],1)))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_net = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 51, 39, 32)        2080      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 18, 64)        32832     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 22, 16, 64)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 22528)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               11534848  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,609,766\n",
      "Trainable params: 11,609,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_net = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 51, 39, 32)        2080      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 24, 18, 64)        32832     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 22, 16, 64)        36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 22528)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               11534848  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,609,766\n",
      "Trainable params: 11,609,766\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "target_q_net.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 0.001 # learning rate\n",
    "TAU = 0.001 # soft update factor\n",
    "\n",
    "STEPS_BETWEEN_LEARNING = 4\n",
    "MINI_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(experiences, gamma, q_net, target_q_net):\n",
    "    '''\n",
    "    y_j = R_j if episode terminates at T_{j+1}, else\n",
    "    y_j = R_j + gamma max_{a'} Q^(s_{j+1}, a')\n",
    "    '''\n",
    "    # unpack experiences into its components\n",
    "    states, actions, rewards, next_states, done_vals = experiences\n",
    "    \n",
    "    # compute max Q^(s,a)\n",
    "    max_qsa = tf.reduce_max(target_q_net(next_states), axis=-1)\n",
    "    \n",
    "    # y = R if episode terminates, else y = R + y max Q^(s,a)\n",
    "    # note: done_vals is boolean, and (1 - done_vals) == 0 if done_vals == True\n",
    "    y_targets = rewards + (gamma * max_qsa * (1 - done_vals))\n",
    "    \n",
    "    q_values = q_net(states)\n",
    "    # tf.gather_nd(params, indices) returns `indices` slices of `params`\n",
    "    q_values = tf.gather_nd(q_values, \n",
    "                            tf.stack([tf.range(q_values.shape[0]),\n",
    "                                      tf.cast(actions, tf.int32)],\n",
    "                                     axis=1\n",
    "                                    ))\n",
    "    \n",
    "    # compute MSE loss\n",
    "    return MSE(y_targets, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def learn(experiences, gamma):\n",
    "    \"\"\"\n",
    "    Fit the Q and Q target networks\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        computed_loss = loss(experiences, gamma, q_net, target_q_net)\n",
    "        \n",
    "    # update q_net\n",
    "    gradients = tape.gradient(computed_loss, q_net.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, q_net.trainable_variables))\n",
    "    \n",
    "    # update target_q_net\n",
    "    for target_weights, q_net_weights in zip(target_q_net.weights, q_net.weights):\n",
    "        target_weights.assign(TAU * q_net_weights + (1.0 - TAU) * target_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100_000\n",
    "GAMMA = 0.995 # discount factor\n",
    "ALPHA = 0.001 # learning rate\n",
    "TAU = 0.001 # soft update factor\n",
    "\n",
    "STEPS_BETWEEN_LEARNING = 4\n",
    "MINI_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10_000 # how many times we play the game\n",
    "max_steps = 1_000_000 # how many actions allowed before 'time out'\n",
    "avg_latest_score_before_save = 250\n",
    "\n",
    "score_history = []\n",
    "point_scores = pandas.DataFrame(columns=['score'])\n",
    "avg_points = 100\n",
    "\n",
    "epsilon = 1.0 # initial epsilon for greedy policy (exploration/exploitation)\n",
    "epsilon_decay = 0.9995\n",
    "epsilon_min = 0.01\n",
    "\n",
    "buffer = deque(maxlen=BUFFER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-14 15:32:13.936595: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m     t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     15\u001b[0m state_qn \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(state, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m q_values \u001b[39m=\u001b[39m q_net(state_qn)\n\u001b[1;32m     17\u001b[0m \u001b[39m# make epsilon greedy choice\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom() \u001b[39m>\u001b[39m epsilon:\n\u001b[1;32m     19\u001b[0m     \u001b[39m# exploitation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/keras/engine/training.py:558\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    556\u001b[0m     layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 558\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/keras/engine/base_layer.py:1070\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[39m# Accept NumPy and scalar inputs by converting to Tensors.\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m   1067\u001b[0m     \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mTensor, np\u001b[39m.\u001b[39mndarray, \u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m))\n\u001b[1;32m   1068\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_list\n\u001b[1;32m   1069\u001b[0m ):\n\u001b[0;32m-> 1070\u001b[0m     inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(\n\u001b[1;32m   1071\u001b[0m         _convert_numpy_or_python_types, inputs\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[1;32m   1073\u001b[0m     input_list \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(inputs)\n\u001b[1;32m   1075\u001b[0m \u001b[39m# Handle `mask` propagation from previous layer to current layer. Masks\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[39m# can be propagated explicitly via the `mask` argument, or implicitly\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m \u001b[39m# via setting the `_keras_mask` attribute on the inputs to a Layer.\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m \u001b[39m# Masks passed explicitly take priority.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/keras/engine/base_layer.py:3725\u001b[0m, in \u001b[0;36m_convert_numpy_or_python_types\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   3723\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_convert_numpy_or_python_types\u001b[39m(x):\n\u001b[1;32m   3724\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, (tf\u001b[39m.\u001b[39mTensor, np\u001b[39m.\u001b[39mndarray, \u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m)):\n\u001b[0;32m-> 3725\u001b[0m         \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(x)\n\u001b[1;32m   3726\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1496\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m   1433\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1434\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1435\u001b[0m     value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1436\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \n\u001b[1;32m   1438\u001b[0m \u001b[39m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[39m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1496\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor_v2(\n\u001b[1;32m   1497\u001b[0m       value, dtype\u001b[39m=\u001b[39;49mdtype, dtype_hint\u001b[39m=\u001b[39;49mdtype_hint, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1502\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, dtype_hint\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1501\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m   \u001b[39mreturn\u001b[39;00m convert_to_tensor(\n\u001b[1;32m   1503\u001b[0m       value\u001b[39m=\u001b[39;49mvalue,\n\u001b[1;32m   1504\u001b[0m       dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1505\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1506\u001b[0m       preferred_dtype\u001b[39m=\u001b[39;49mdtype_hint,\n\u001b[1;32m   1507\u001b[0m       as_ref\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1644\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:48\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[1;32m     47\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    269\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    282\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    306\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/AI-Mac/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_begin = time.time()\n",
    "\n",
    "# clone q_net into target_q_net\n",
    "target_q_net.set_weights(q_net.get_weights())\n",
    "\n",
    "for i in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        if(t <= max_steps):\n",
    "            t += 1\n",
    "        state_qn = np.expand_dims(state, axis=0)\n",
    "        q_values = q_net(state_qn)\n",
    "        # make epsilon greedy choice\n",
    "        if random.random() > epsilon:\n",
    "            # exploitation\n",
    "            action = np.argmax(q_values.numpy()[0])\n",
    "        else:\n",
    "            # exploration\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        # take action and update buffer\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        buffer.append(experience(state, action, reward, next_state, done))\n",
    "        \n",
    "        # check whether this is a learning step\n",
    "        if (t + 1) % STEPS_BETWEEN_LEARNING == 0 and len(buffer) > MINI_BATCH_SIZE:\n",
    "            # this is a learning step\n",
    "            experiences = random.sample(buffer, k=MINI_BATCH_SIZE)\n",
    "            states = tf.convert_to_tensor(\n",
    "                np.array([e.state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            actions = tf.convert_to_tensor(\n",
    "                np.array([e.action for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            rewards = tf.convert_to_tensor(\n",
    "                np.array([e.reward for e in experiences if e is not None]), \n",
    "                dtype=tf.float32)\n",
    "            next_states = tf.convert_to_tensor(\n",
    "                np.array([e.next_state for e in experiences if e is not None]),\n",
    "                dtype=tf.float32)\n",
    "            done_vals = tf.convert_to_tensor(\n",
    "                np.array([e.done for e in experiences if e is not None]).astype(np.uint8),\n",
    "                dtype=tf.float32)\n",
    "            experiences = states, actions, rewards, next_states, done_vals\n",
    "            learn(experiences, GAMMA)\n",
    "            \n",
    "        state = next_state.copy()\n",
    "        score += reward\n",
    "        \n",
    "        if done: break\n",
    "\n",
    "    point_scores = pandas.concat([point_scores, pandas.DataFrame({'score': [score]})], ignore_index=True)    \n",
    "    score_history.append(score)\n",
    "    avg_latest_score = np.mean(score_history[-avg_points:])\n",
    "\n",
    "    # update epsilon\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # display status\n",
    "    status_message = f'\\rEpisode {i + 1} | Total score average of last {avg_points} episodes: {avg_latest_score:.2f}'\n",
    "    print(status_message, end='')\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(status_message)\n",
    "\n",
    "    if avg_latest_score >= avg_latest_score_before_save:\n",
    "        print(f\"\\n\\nEnvironment solved in {i+1} episodes!\")\n",
    "        q_net.save('crazy_astronaut.h5')\n",
    "            \n",
    "time_end = time.time()\n",
    "runtime = time_end - time_begin\n",
    "print(f'\\nTotal runtime: {runtime:.2f} sec. ({(runtime/60):.2f} min.)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_scores.tail()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the real graf we need to developmen of the top values (400 and above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(point_scores)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "\n",
    "for index in score_history:\n",
    "    if(index > 300):\n",
    "        list.append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(list)\n",
    "plt.xlabel(\"Episode\", fontsize=14)\n",
    "plt.ylabel(\"Sum of rewards\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion, training an AI on a game like space invaders takes a long time, due to the fact as the agent surives longer it requires time and energy.\n",
    "\n",
    "There could utilised different strategies to optimize this process and lowering dimension og set scope to someling like (highest score) in shortes amount of time, as this would like triggers the AI to seek to kill the aliens faster and this would also transition to surviving longer when competing. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Agent with Keras-RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Setup (this need to point the trained agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(env):\n",
    "    \"\"\"Random agent that samples actions from the environment's action space.\"\"\"\n",
    "    return env.action_space.sample()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def get_agent_scores(num_episodes = 1):\n",
    "    scores = pandas.DataFrame(columns=['score'])\n",
    "    total_reward = 0\n",
    "    #highest_score = 0\n",
    "    \n",
    "    for _ in range(0, num_episodes + 1):\n",
    "        #if(_ > 1 and total_reward > 0):\n",
    "        #    print(f'Game Episode {_ - 1}: Score: {total_reward}')\n",
    "        env.close()\n",
    "        total_reward = 0\n",
    "        observation, info = env.reset()  # Getting start stats from the game\n",
    "        lives = info['lives']  # Initialize lives inside the loop\n",
    "        while lives != 0:\n",
    "            # For demonstration purposes, we use a random agent\n",
    "            action = agent(env)   # Just a random agent\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            lives = info['lives']\n",
    "            total_reward = total_reward + reward\n",
    "\n",
    "        # Update highest score if current total reward is higher\n",
    "        #if total_reward > highest_score:\n",
    "        #    highest_score = total_reward\n",
    "\n",
    "        scores = pandas.concat([scores, pandas.DataFrame({'score': [total_reward]})], ignore_index=True)\n",
    "\n",
    "\n",
    "    \"\"\" # Print the highest score achieved\n",
    "    print(f\"Highest Score: {highest_score}\") \"\"\"\n",
    "\n",
    "    # Close the environment to finalize the video recording\n",
    "    scores = scores.drop(scores.index[0])\n",
    "    return scores\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "\n",
    "scores = get_agent_scores(num_episodes = num_episodes)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Statistics of Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_start = 0\n",
    "index_end = num_episodes-1\n",
    "\n",
    "print(f\"Game Episode 1: Score {scores.iloc[index_start]['score']}\\nGame Episode {num_episodes}: Score {scores.iloc[index_end]['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Lowest Score: {scores['score'].min()}\\nHigh Score: {scores['score'].max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mode (most frequent element)\n",
    "It is possible for a sample to have multiple modes. In statistics, a mode refers to the value or values in a dataset that occur most frequently. If there are multiple values with the same highest frequency, the dataset is considered multimodal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mode()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median (middle value of the data set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean (average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance (Dissimilarity between samples)\n",
    "\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "Note: The number is high due there is a lot of Dissimilarities between each score."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma=\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}\\text{ for samples (degrees of freedom is 0)}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.var(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Deviation ( variability within a sample)\n",
    "For samples degrees of freedom (ddof) is 0.\n",
    "\n",
    "$$SD_0=\\sqrt{\\frac{\\sum\\left(x_i-\\mu\\right)^2}{n}}\\text{ for samples (degrees of freedom is 0)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scores.std(ddof=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = pandas_profiling.ProfileReport(scores).to_file('test_agent_scores_report.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
