{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99f0839-f6c3-4e25-9e27-e4b847ac1040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import import_ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bd29c74-5388-4a26-9b49-10c0463f60b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from metrics.ipynb\n",
      "importing Jupyter notebook from agent.ipynb\n",
      "importing Jupyter notebook from neural.ipynb\n",
      "importing Jupyter notebook from wrappers.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import random, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "from metrics import MetricLogger\n",
    "from agent import SpaceInvader\n",
    "from wrappers import ResizeObservation, SkipFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f209af9-7384-4ddd-a17e-1d4db42ac291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fa832df-e22f-473e-9fea-b03a339da02c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ce27c12-5618-4c12-a7df-b30871b5bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gymnasium.make(\"ALE/SpaceInvaders-v5\", mode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b98f241-7142-41ef-9b34-b2d20c5cbd56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af46e8db-4556-4a68-a792-17a28cafdb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gymnasium.wrappers.frame_stack.LazyFrames at 0x28504d760>,\n",
       " {'lives': 3, 'episode_frame_number': 3, 'frame_number': 3})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3b1847c-db8a-4293-a902-c8619ea37f32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1afa306b-d5be-4e54-b069-7472fc60d409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83e1ecbb-36d2-433e-8af4-4e00193799e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = None\n",
    "spaceInvader = SpaceInvader(state_dim=(4,84,84), action_dim=env.action_space.n, save_dir=save_dir, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b722f577-a9d0-4146-aa19-940ca757dadf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = MetricLogger(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "00ed0b31-4fc5-49a7-ab18-46a2ef916070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "episodes = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf4b51d5-6d30-4612-94bb-5292aa3911dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4d087ef-7c46-4ea6-854a-04a52f0789fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = env.step(action)\n",
    "next_state = result[0]\n",
    "reward = result[1]\n",
    "done = result[2]\n",
    "info = result[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f44f816a-6778-47f4-bf60-50f1a07ab632",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gymnasium.wrappers.frame_stack.LazyFrames at 0x2b01bd0d0>,\n",
       " 0.0,\n",
       " False,\n",
       " False,\n",
       " {'lives': 3, 'episode_frame_number': 35, 'frame_number': 35})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = env.step(action)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d621a36f-f5b2-4ddd-a55d-943cc69f254c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<gymnasium.wrappers.frame_stack.LazyFrames at 0x2b0ba8fe0>,\n",
       " {'lives': 3, 'episode_frame_number': 3, 'frame_number': 38})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa91fc2-80b9-49ad-b7d3-c791a2d53942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:73: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 124 - Epsilon 0.9999690004766157 - Mean Reward 115.0 - Mean Length 124.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 3.556 - Time 2023-05-11T17:10:20\n",
      "Episode 20 - Step 2500 - Epsilon 0.9993751951936526 - Mean Reward 96.429 - Mean Length 119.048 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 12.862 - Time 2023-05-11T17:10:33\n",
      "Episode 40 - Step 5241 - Epsilon 0.9986906078390085 - Mean Reward 112.683 - Mean Length 127.829 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.12 - Time 2023-05-11T17:10:48\n",
      "Episode 60 - Step 8158 - Epsilon 0.9979625781122371 - Mean Reward 135.164 - Mean Length 133.738 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.692 - Time 2023-05-11T17:11:04\n",
      "Episode 80 - Step 10788 - Epsilon 0.9973066333005305 - Mean Reward 134.506 - Mean Length 133.185 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.375 - Time 2023-05-11T17:11:18\n",
      "Episode 100 - Step 13388 - Epsilon 0.9966585945432022 - Mean Reward 128.45 - Mean Length 132.64 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.103 - Time 2023-05-11T17:11:32\n",
      "Episode 120 - Step 15911 - Epsilon 0.9960301502724976 - Mean Reward 129.4 - Mean Length 134.11 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 13.637 - Time 2023-05-11T17:11:46\n",
      "Episode 140 - Step 18357 - Epsilon 0.9954212639453792 - Mean Reward 130.1 - Mean Length 131.16 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 13.134 - Time 2023-05-11T17:11:59\n",
      "Episode 160 - Step 20891 - Epsilon 0.9947908641959843 - Mean Reward 121.0 - Mean Length 127.33 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.103 - Time 2023-05-11T17:12:13\n",
      "Episode 180 - Step 23642 - Epsilon 0.9941069319077185 - Mean Reward 120.65 - Mean Length 128.54 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.095 - Time 2023-05-11T17:12:29\n",
      "Episode 200 - Step 26409 - Epsilon 0.993419496145803 - Mean Reward 134.15 - Mean Length 130.21 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.995 - Time 2023-05-11T17:12:46\n",
      "Episode 220 - Step 28794 - Epsilon 0.9928273462493574 - Mean Reward 135.0 - Mean Length 128.83 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.583 - Time 2023-05-11T17:13:01\n",
      "Episode 240 - Step 31415 - Epsilon 0.992177009139309 - Mean Reward 131.4 - Mean Length 130.58 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.009 - Time 2023-05-11T17:13:17\n",
      "Episode 260 - Step 34203 - Epsilon 0.9914857026257337 - Mean Reward 135.55 - Mean Length 133.12 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 17.013 - Time 2023-05-11T17:13:34\n",
      "Episode 280 - Step 36722 - Epsilon 0.990861510989359 - Mean Reward 131.3 - Mean Length 130.8 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.542 - Time 2023-05-11T17:13:49\n",
      "Episode 300 - Step 39769 - Epsilon 0.9901070095461668 - Mean Reward 128.8 - Mean Length 133.6 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 18.396 - Time 2023-05-11T17:14:07\n",
      "Episode 320 - Step 42238 - Epsilon 0.9894960544934697 - Mean Reward 136.25 - Mean Length 134.44 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.918 - Time 2023-05-11T17:14:22\n",
      "Episode 340 - Step 44798 - Epsilon 0.9888629795449599 - Mean Reward 144.05 - Mean Length 133.83 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.418 - Time 2023-05-11T17:14:38\n",
      "Episode 360 - Step 47500 - Epsilon 0.9881952280771665 - Mean Reward 142.0 - Mean Length 132.97 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.411 - Time 2023-05-11T17:14:54\n",
      "Episode 380 - Step 49834 - Epsilon 0.987618784283281 - Mean Reward 142.5 - Mean Length 131.12 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.571 - Time 2023-05-11T17:15:09\n",
      "Episode 400 - Step 52465 - Epsilon 0.986969391539105 - Mean Reward 138.45 - Mean Length 126.96 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.263 - Time 2023-05-11T17:15:25\n",
      "Episode 420 - Step 55201 - Epsilon 0.9862945352180132 - Mean Reward 137.4 - Mean Length 129.63 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.799 - Time 2023-05-11T17:15:42\n",
      "Episode 440 - Step 57604 - Epsilon 0.985702196643369 - Mean Reward 131.25 - Mean Length 128.06 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.9 - Time 2023-05-11T17:15:57\n",
      "Episode 460 - Step 60199 - Epsilon 0.985062929647952 - Mean Reward 134.05 - Mean Length 126.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.229 - Time 2023-05-11T17:16:13\n",
      "Episode 480 - Step 62691 - Epsilon 0.9844494264920843 - Mean Reward 133.3 - Mean Length 128.57 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.405 - Time 2023-05-11T17:16:28\n",
      "Episode 500 - Step 65215 - Epsilon 0.9838284347697522 - Mean Reward 127.25 - Mean Length 127.5 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.704 - Time 2023-05-11T17:16:44\n",
      "Episode 520 - Step 67802 - Epsilon 0.9831923493662557 - Mean Reward 129.8 - Mean Length 126.01 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.652 - Time 2023-05-11T17:17:01\n",
      "Episode 540 - Step 70148 - Epsilon 0.9826158760485383 - Mean Reward 127.35 - Mean Length 125.44 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.661 - Time 2023-05-11T17:17:15\n",
      "Episode 560 - Step 72706 - Epsilon 0.9819876939998142 - Mean Reward 121.85 - Mean Length 125.07 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.845 - Time 2023-05-11T17:17:31\n",
      "Episode 580 - Step 75207 - Epsilon 0.981373898025294 - Mean Reward 132.6 - Mean Length 125.16 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.515 - Time 2023-05-11T17:17:47\n",
      "Episode 600 - Step 78014 - Epsilon 0.9806854603900103 - Mean Reward 142.25 - Mean Length 127.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 17.292 - Time 2023-05-11T17:18:04\n",
      "Episode 620 - Step 80501 - Epsilon 0.9800759086427696 - Mean Reward 140.6 - Mean Length 126.99 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.429 - Time 2023-05-11T17:18:19\n",
      "Episode 640 - Step 83209 - Epsilon 0.979412621717572 - Mean Reward 150.0 - Mean Length 130.61 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.769 - Time 2023-05-11T17:18:36\n",
      "Episode 660 - Step 85804 - Epsilon 0.9787774337611175 - Mean Reward 148.2 - Mean Length 130.98 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.374 - Time 2023-05-11T17:18:53\n",
      "Episode 680 - Step 88740 - Epsilon 0.9780592746315119 - Mean Reward 149.8 - Mean Length 135.33 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 18.505 - Time 2023-05-11T17:19:11\n",
      "Episode 700 - Step 91501 - Epsilon 0.9773844020749299 - Mean Reward 150.65 - Mean Length 134.87 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 17.237 - Time 2023-05-11T17:19:28\n",
      "Episode 720 - Step 93930 - Epsilon 0.9767910654925749 - Mean Reward 145.6 - Mean Length 134.29 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.005 - Time 2023-05-11T17:19:43\n",
      "Episode 740 - Step 96282 - Epsilon 0.9762168811009664 - Mean Reward 138.3 - Mean Length 130.73 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 14.442 - Time 2023-05-11T17:19:58\n",
      "Episode 760 - Step 98880 - Epsilon 0.9755830340214923 - Mean Reward 134.2 - Mean Length 130.76 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 16.11 - Time 2023-05-11T17:20:14\n",
      "Episode 780 - Step 101299 - Epsilon 0.9749932284682916 - Mean Reward 123.35 - Mean Length 125.59 - Mean Loss 0.117 - Mean Q Value 0.007 - Time Delta 60.781 - Time 2023-05-11T17:21:15\n",
      "Episode 800 - Step 104151 - Epsilon 0.9742983059787381 - Mean Reward 121.45 - Mean Length 126.5 - Mean Loss 0.323 - Mean Q Value 0.022 - Time Delta 115.934 - Time 2023-05-11T17:23:11\n",
      "Episode 820 - Step 106869 - Epsilon 0.9736364950727271 - Mean Reward 128.65 - Mean Length 129.39 - Mean Loss 0.511 - Mean Q Value 0.035 - Time Delta 109.834 - Time 2023-05-11T17:25:00\n",
      "Episode 840 - Step 109355 - Epsilon 0.9730315679159255 - Mean Reward 133.2 - Mean Length 130.73 - Mean Loss 0.721 - Mean Q Value 0.049 - Time Delta 105.394 - Time 2023-05-11T17:26:46\n",
      "Episode 860 - Step 112052 - Epsilon 0.9723757224259431 - Mean Reward 140.9 - Mean Length 131.72 - Mean Loss 0.923 - Mean Q Value 0.077 - Time Delta 113.973 - Time 2023-05-11T17:28:40\n",
      "Episode 880 - Step 114251 - Epsilon 0.9718413057169146 - Mean Reward 135.45 - Mean Length 129.52 - Mean Loss 1.019 - Mean Q Value 0.104 - Time Delta 92.396 - Time 2023-05-11T17:30:12\n",
      "Episode 900 - Step 117085 - Epsilon 0.9711529999268008 - Mean Reward 138.3 - Mean Length 129.34 - Mean Loss 1.001 - Mean Q Value 0.125 - Time Delta 118.446 - Time 2023-05-11T17:32:11\n",
      "Episode 920 - Step 119331 - Epsilon 0.9706078505141427 - Mean Reward 130.4 - Mean Length 124.62 - Mean Loss 1.014 - Mean Q Value 0.146 - Time Delta 94.288 - Time 2023-05-11T17:33:45\n",
      "Episode 940 - Step 121675 - Epsilon 0.9700392408614857 - Mean Reward 121.25 - Mean Length 123.2 - Mean Loss 1.0 - Mean Q Value 0.176 - Time Delta 98.144 - Time 2023-05-11T17:35:23\n",
      "Episode 960 - Step 124130 - Epsilon 0.9694440618671594 - Mean Reward 114.6 - Mean Length 120.78 - Mean Loss 0.992 - Mean Q Value 0.198 - Time Delta 103.056 - Time 2023-05-11T17:37:06\n",
      "Episode 980 - Step 126547 - Epsilon 0.9688584521646434 - Mean Reward 121.4 - Mean Length 122.96 - Mean Loss 0.98 - Mean Q Value 0.213 - Time Delta 100.428 - Time 2023-05-11T17:38:47\n",
      "Episode 1000 - Step 129263 - Epsilon 0.9682008204845485 - Mean Reward 119.75 - Mean Length 121.78 - Mean Loss 0.999 - Mean Q Value 0.229 - Time Delta 111.289 - Time 2023-05-11T17:40:38\n",
      "Episode 1020 - Step 131720 - Epsilon 0.9676062856713632 - Mean Reward 124.15 - Mean Length 123.89 - Mean Loss 0.996 - Mean Q Value 0.256 - Time Delta 101.177 - Time 2023-05-11T17:42:19\n",
      "Episode 1040 - Step 134452 - Epsilon 0.9669456361330757 - Mean Reward 135.25 - Mean Length 127.77 - Mean Loss 0.996 - Mean Q Value 0.278 - Time Delta 113.097 - Time 2023-05-11T17:44:12\n",
      "Episode 1060 - Step 136990 - Epsilon 0.9663323036505017 - Mean Reward 139.8 - Mean Length 128.6 - Mean Loss 0.999 - Mean Q Value 0.296 - Time Delta 106.237 - Time 2023-05-11T17:45:58\n",
      "Episode 1080 - Step 139884 - Epsilon 0.9656334149948084 - Mean Reward 148.9 - Mean Length 133.37 - Mean Loss 1.0 - Mean Q Value 0.312 - Time Delta 119.384 - Time 2023-05-11T17:47:58\n",
      "Episode 1100 - Step 142055 - Epsilon 0.9651094595945108 - Mean Reward 135.05 - Mean Length 127.92 - Mean Loss 0.981 - Mean Q Value 0.341 - Time Delta 90.625 - Time 2023-05-11T17:49:28\n",
      "Episode 1120 - Step 144771 - Epsilon 0.9644543726166654 - Mean Reward 138.15 - Mean Length 130.51 - Mean Loss 0.986 - Mean Q Value 0.36 - Time Delta 114.322 - Time 2023-05-11T17:51:23\n",
      "Episode 1140 - Step 147270 - Epsilon 0.9638520178521574 - Mean Reward 133.0 - Mean Length 128.18 - Mean Loss 0.99 - Mean Q Value 0.374 - Time Delta 104.366 - Time 2023-05-11T17:53:07\n",
      "Episode 1160 - Step 149955 - Epsilon 0.9632052492002514 - Mean Reward 133.35 - Mean Length 129.65 - Mean Loss 0.996 - Mean Q Value 0.39 - Time Delta 110.963 - Time 2023-05-11T17:54:58\n",
      "Episode 1180 - Step 152506 - Epsilon 0.9625911608139741 - Mean Reward 135.35 - Mean Length 126.22 - Mean Loss 1.014 - Mean Q Value 0.422 - Time Delta 106.176 - Time 2023-05-11T17:56:44\n",
      "Episode 1200 - Step 155265 - Epsilon 0.9619274424039445 - Mean Reward 149.05 - Mean Length 132.1 - Mean Loss 1.021 - Mean Q Value 0.44 - Time Delta 115.411 - Time 2023-05-11T17:58:40\n",
      "Episode 1220 - Step 157759 - Epsilon 0.9613278675054734 - Mean Reward 146.9 - Mean Length 129.88 - Mean Loss 1.015 - Mean Q Value 0.455 - Time Delta 103.939 - Time 2023-05-11T18:00:24\n",
      "Episode 1240 - Step 159778 - Epsilon 0.9608427596430424 - Mean Reward 138.2 - Mean Length 125.08 - Mean Loss 1.005 - Mean Q Value 0.471 - Time Delta 84.153 - Time 2023-05-11T18:01:48\n",
      "Episode 1260 - Step 162527 - Epsilon 0.9601826472309805 - Mean Reward 143.8 - Mean Length 125.72 - Mean Loss 1.001 - Mean Q Value 0.508 - Time Delta 113.091 - Time 2023-05-11T18:03:41\n",
      "Episode 1280 - Step 164803 - Epsilon 0.959636458641745 - Mean Reward 123.7 - Mean Length 122.97 - Mean Loss 0.994 - Mean Q Value 0.536 - Time Delta 92.465 - Time 2023-05-11T18:05:13\n",
      "Episode 1300 - Step 167543 - Epsilon 0.9589793326766772 - Mean Reward 116.9 - Mean Length 122.78 - Mean Loss 1.0 - Mean Q Value 0.56 - Time Delta 114.299 - Time 2023-05-11T18:07:08\n",
      "Episode 1320 - Step 169908 - Epsilon 0.9584125036606398 - Mean Reward 111.55 - Mean Length 121.49 - Mean Loss 1.008 - Mean Q Value 0.585 - Time Delta 97.374 - Time 2023-05-11T18:08:45\n",
      "Episode 1340 - Step 172677 - Epsilon 0.9577492721094892 - Mean Reward 122.85 - Mean Length 128.99 - Mean Loss 1.003 - Mean Q Value 0.62 - Time Delta 111.51 - Time 2023-05-11T18:10:36\n",
      "Episode 1360 - Step 174995 - Epsilon 0.9571944171216274 - Mean Reward 112.05 - Mean Length 124.68 - Mean Loss 1.009 - Mean Q Value 0.632 - Time Delta 95.548 - Time 2023-05-11T18:12:12\n",
      "Episode 1380 - Step 177545 - Epsilon 0.9565844000679712 - Mean Reward 120.5 - Mean Length 127.42 - Mean Loss 0.995 - Mean Q Value 0.64 - Time Delta 105.949 - Time 2023-05-11T18:13:58\n",
      "Episode 1400 - Step 180119 - Epsilon 0.9559690359441636 - Mean Reward 123.5 - Mean Length 125.76 - Mean Loss 0.985 - Mean Q Value 0.653 - Time Delta 107.134 - Time 2023-05-11T18:15:45\n",
      "Episode 1420 - Step 182636 - Epsilon 0.9553676815740032 - Mean Reward 133.2 - Mean Length 127.28 - Mean Loss 0.98 - Mean Q Value 0.705 - Time Delta 104.977 - Time 2023-05-11T18:17:30\n",
      "Episode 1440 - Step 185087 - Epsilon 0.9547824592696784 - Mean Reward 127.0 - Mean Length 124.1 - Mean Loss 0.991 - Mean Q Value 0.744 - Time Delta 101.046 - Time 2023-05-11T18:19:11\n",
      "Episode 1460 - Step 187913 - Epsilon 0.9541081436078769 - Mean Reward 137.45 - Mean Length 129.18 - Mean Loss 0.974 - Mean Q Value 0.784 - Time Delta 117.808 - Time 2023-05-11T18:21:09\n",
      "Episode 1480 - Step 190565 - Epsilon 0.9534757794814046 - Mean Reward 140.9 - Mean Length 130.2 - Mean Loss 0.978 - Mean Q Value 0.831 - Time Delta 103.27 - Time 2023-05-11T18:22:52\n",
      "Episode 1500 - Step 192835 - Epsilon 0.9529348354162511 - Mean Reward 133.65 - Mean Length 127.16 - Mean Loss 0.992 - Mean Q Value 0.905 - Time Delta 84.529 - Time 2023-05-11T18:24:17\n",
      "Episode 1520 - Step 195179 - Epsilon 0.9523765791179073 - Mean Reward 128.65 - Mean Length 125.43 - Mean Loss 0.98 - Mean Q Value 0.938 - Time Delta 86.576 - Time 2023-05-11T18:25:43\n",
      "Episode 1540 - Step 197929 - Epsilon 0.9517220451596873 - Mean Reward 137.95 - Mean Length 128.42 - Mean Loss 0.987 - Mean Q Value 0.974 - Time Delta 100.313 - Time 2023-05-11T18:27:24\n",
      "Episode 1560 - Step 201089 - Epsilon 0.9509704815566711 - Mean Reward 151.05 - Mean Length 131.76 - Mean Loss 0.992 - Mean Q Value 1.02 - Time Delta 116.319 - Time 2023-05-11T18:29:20\n",
      "Episode 1580 - Step 203414 - Epsilon 0.9504178905074463 - Mean Reward 149.1 - Mean Length 128.49 - Mean Loss 0.982 - Mean Q Value 1.086 - Time Delta 87.375 - Time 2023-05-11T18:30:47\n",
      "Episode 1600 - Step 206397 - Epsilon 0.9497093804954422 - Mean Reward 164.85 - Mean Length 135.62 - Mean Loss 0.989 - Mean Q Value 1.125 - Time Delta 111.327 - Time 2023-05-11T18:32:39\n",
      "Episode 1620 - Step 209064 - Epsilon 0.9490763727391933 - Mean Reward 165.25 - Mean Length 138.85 - Mean Loss 0.995 - Mean Q Value 1.167 - Time Delta 99.7 - Time 2023-05-11T18:34:18\n",
      "Episode 1640 - Step 211576 - Epsilon 0.9484805398136686 - Mean Reward 159.25 - Mean Length 136.47 - Mean Loss 0.994 - Mean Q Value 1.231 - Time Delta 93.706 - Time 2023-05-11T18:35:52\n",
      "Episode 1660 - Step 214280 - Epsilon 0.9478395835561699 - Mean Reward 145.5 - Mean Length 131.91 - Mean Loss 1.007 - Mean Q Value 1.297 - Time Delta 100.537 - Time 2023-05-11T18:37:33\n",
      "Episode 1680 - Step 217107 - Epsilon 0.947169934511795 - Mean Reward 150.3 - Mean Length 136.93 - Mean Loss 1.011 - Mean Q Value 1.336 - Time Delta 105.505 - Time 2023-05-11T18:39:18\n",
      "Episode 1700 - Step 219637 - Epsilon 0.9465710388739834 - Mean Reward 139.2 - Mean Length 132.4 - Mean Loss 0.999 - Mean Q Value 1.377 - Time Delta 95.026 - Time 2023-05-11T18:40:53\n",
      "Episode 1720 - Step 222334 - Epsilon 0.9459330283846393 - Mean Reward 140.2 - Mean Length 132.7 - Mean Loss 1.002 - Mean Q Value 1.449 - Time Delta 100.975 - Time 2023-05-11T18:42:34\n",
      "Episode 1740 - Step 225199 - Epsilon 0.9452557463488878 - Mean Reward 148.4 - Mean Length 136.23 - Mean Loss 1.013 - Mean Q Value 1.5 - Time Delta 106.536 - Time 2023-05-11T18:44:21\n",
      "Episode 1760 - Step 228337 - Epsilon 0.9445144839213149 - Mean Reward 158.6 - Mean Length 140.57 - Mean Loss 1.019 - Mean Q Value 1.542 - Time Delta 118.229 - Time 2023-05-11T18:46:19\n",
      "Episode 1780 - Step 231031 - Epsilon 0.9438785675057069 - Mean Reward 159.75 - Mean Length 139.24 - Mean Loss 1.041 - Mean Q Value 1.593 - Time Delta 101.591 - Time 2023-05-11T18:48:00\n",
      "Episode 1800 - Step 233417 - Epsilon 0.9433157117581715 - Mean Reward 158.3 - Mean Length 137.8 - Mean Loss 1.051 - Mean Q Value 1.664 - Time Delta 90.36 - Time 2023-05-11T18:49:31\n",
      "Episode 1820 - Step 236205 - Epsilon 0.9426584497072616 - Mean Reward 161.05 - Mean Length 138.71 - Mean Loss 1.055 - Mean Q Value 1.701 - Time Delta 103.497 - Time 2023-05-11T18:51:14\n",
      "Episode 1840 - Step 238872 - Epsilon 0.9420301415927889 - Mean Reward 160.2 - Mean Length 136.73 - Mean Loss 1.058 - Mean Q Value 1.736 - Time Delta 100.936 - Time 2023-05-11T18:52:55\n",
      "Episode 1860 - Step 241204 - Epsilon 0.9414810980132872 - Mean Reward 144.9 - Mean Length 128.67 - Mean Loss 1.062 - Mean Q Value 1.787 - Time Delta 86.839 - Time 2023-05-11T18:54:22\n",
      "Episode 1880 - Step 244069 - Epsilon 0.9408070035313872 - Mean Reward 144.7 - Mean Length 130.38 - Mean Loss 1.062 - Mean Q Value 1.843 - Time Delta 104.669 - Time 2023-05-11T18:56:07\n",
      "Episode 1900 - Step 246522 - Epsilon 0.9402302304354218 - Mean Reward 144.35 - Mean Length 131.05 - Mean Loss 1.069 - Mean Q Value 1.875 - Time Delta 90.929 - Time 2023-05-11T18:57:38\n",
      "Episode 1920 - Step 249036 - Epsilon 0.9396394813240211 - Mean Reward 135.4 - Mean Length 128.31 - Mean Loss 1.077 - Mean Q Value 1.902 - Time Delta 96.072 - Time 2023-05-11T18:59:14\n",
      "Episode 1940 - Step 251873 - Epsilon 0.9389732782187262 - Mean Reward 134.9 - Mean Length 130.01 - Mean Loss 1.072 - Mean Q Value 1.954 - Time Delta 109.41 - Time 2023-05-11T19:01:03\n",
      "Episode 1960 - Step 254342 - Epsilon 0.9383938757267289 - Mean Reward 133.15 - Mean Length 131.38 - Mean Loss 1.066 - Mean Q Value 1.999 - Time Delta 95.931 - Time 2023-05-11T19:02:39\n",
      "Episode 1980 - Step 257073 - Epsilon 0.9377534058932001 - Mean Reward 134.35 - Mean Length 130.04 - Mean Loss 1.07 - Mean Q Value 2.023 - Time Delta 104.142 - Time 2023-05-11T19:04:23\n"
     ]
    }
   ],
   "source": [
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state = state[0]\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # 3. Show environment (the visual) [WIP]\n",
    "        # env.render()\n",
    "\n",
    "        # 4. Run agent on the state\n",
    "        action = spaceInvader.act(state)\n",
    "        # 5. Agent performs action\n",
    "        result = env.step(action)\n",
    "        next_state = result[0]\n",
    "        reward = result[1]\n",
    "        done = result[2]\n",
    "        info = result[3]\n",
    "        \n",
    "        # 6. Remember\n",
    "        spaceInvader.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 7. Learn\n",
    "        q, loss = spaceInvader.learn()\n",
    "\n",
    "        # 8. Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 9. Update state\n",
    "        state = next_state\n",
    "        # 10. Check if end of game\n",
    "        if done or info:\n",
    "            break\n",
    "    \n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=spaceInvader.exploration_rate,\n",
    "            step=spaceInvader.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031d85f-4787-4b25-9beb-1a83ce9d8b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e43b40d-b3fe-4eae-b185-10be0215cf58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
